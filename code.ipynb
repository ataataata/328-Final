{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from scipy.signal import butter, filtfilt, find_peaks\n",
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding helpers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_magnitude(data):\n",
    "\n",
    "    # Calculate magnitude\n",
    "    data['accel_mag'] = np.sqrt(data['x']**2 + data['y']**2 + data['z']**2) # absolute accel magnitude\n",
    "    data['accel_mag'] = data['accel_mag'] - data['accel_mag'].mean() # detrend: \"remove gravity\"\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(data,sampling_rate):\n",
    "    from scipy.signal import butter, filtfilt, find_peaks\n",
    "\n",
    "    # Low pass filter\n",
    "    cutoff = 5 # Hz\n",
    "    order = 2\n",
    "    b, a = butter(order, cutoff/(sampling_rate/2), btype='lowpass')\n",
    "    data['filtered_accel_mag'] = filtfilt(b, a, data['accel_mag'])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(window):\n",
    "    features = {}\n",
    "    features['avg'] = window['filtered_accel_mag'].mean()\n",
    "    features['max'] = window['filtered_accel_mag'].quantile(1)\n",
    "    features['med'] = window['filtered_accel_mag'].quantile(0.5)\n",
    "    features['min'] = window['filtered_accel_mag'].quantile(0)\n",
    "    features['q25'] = window['filtered_accel_mag'].quantile(0.25)\n",
    "    features['q75'] = window['filtered_accel_mag'].quantile(0.75)\n",
    "    features['std'] = window['filtered_accel_mag'].std()\n",
    "    df = pd.DataFrame()\n",
    "    df = df._append(features,ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_tree(frames):\n",
    "    # Extract feature columns\n",
    "    X = frames[['avg', 'max', 'med', 'min', 'q25', 'q75', 'std']]\n",
    "\n",
    "    # Extract target column\n",
    "    y = frames['activity']\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Create model\n",
    "    dt_model = DecisionTreeClassifier(criterion='entropy',max_depth=5).fit(X_train, y_train)\n",
    "    dt_pred = dt_model.predict(X_test)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    acc = dt_model.score(X_test, y_test)\n",
    "    dt_cm = confusion_matrix(y_test, dt_pred, labels=dt_model.classes_)\n",
    "    print(classification_report(y_test, dt_pred))\n",
    "    print(\"Accuracy on test set:\", acc)\n",
    "\n",
    "    return dt_model,dt_cm,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calulating the orientation for sleep tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_orientation(data):\n",
    "    data['pitch'] = np.arctan2(data['y'], np.sqrt(data['x']**2 + data['z']**2))\n",
    "    data['roll'] = np.arctan2(-data['x'], data['z'])    \n",
    "    data['pitch'] = np.degrees(data['pitch'])\n",
    "    data['roll'] = np.degrees(data['roll'])\n",
    "\n",
    "\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding extra feauters for sleep position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_position_features(window):\n",
    "    features = {}\n",
    "    \n",
    "    base_features = add_features(window)\n",
    "    \n",
    "    features['pitch_mean'] = window['pitch'].mean()\n",
    "    features['pitch_std'] = window['pitch'].std()\n",
    "    features['roll_mean'] = window['roll'].mean()\n",
    "    features['roll_std'] = window['roll'].std()\n",
    "    \n",
    "    features['stability'] = window['filtered_accel_mag'].std() / window['filtered_accel_mag'].mean()\n",
    "    features['movement_intensity'] = np.sum(np.abs(np.diff(window['filtered_accel_mag'])))\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df = df._append({**base_features.iloc[0], **features}, ignore_index=True)\n",
    "    df= df._append()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_position_features(data, window_sec, sample_rate, position):\n",
    "    data = pd.read_csv('your_file.csv')\n",
    "    data['timestamp'] = pd.to_datetime(data.index, unit='ns')\n",
    "    data.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    data = calc_orientation(data)\n",
    "    \n",
    "    window_size = f'{window_sec}s'\n",
    "    \n",
    "    resampled_data = data.resample(window_size)\n",
    "    \n",
    "    features_df = pd.DataFrame()\n",
    "    \n",
    "    for timestamp, window in resampled_data:\n",
    "        if not window.empty:\n",
    "            features = add_position_features(window)\n",
    "            \n",
    "            features['position'] = position\n",
    "            \n",
    "            features_df = pd.concat([features_df, features], ignore_index=True)\n",
    "    \n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_position_classifier(frames):\n",
    "    features = ['avg', 'max', 'med', 'min', 'q25', 'q75', 'std',\n",
    "                'pitch_mean', 'pitch_std', 'roll_mean', 'roll_std',\n",
    "                'stability', 'movement_intensity']\n",
    "    \n",
    "    X = frames[features]\n",
    "    y = frames['position']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    dt_model = DecisionTreeClassifier(\n",
    "        criterion='entropy',\n",
    "        max_depth=6,  \n",
    "        min_samples_leaf=5  \n",
    "    ).fit(X_train, y_train)\n",
    "    \n",
    "    dt_pred = dt_model.predict(X_test)\n",
    "\n",
    "    acc = dt_model.score(X_test, y_test)\n",
    "    dt_cm = confusion_matrix(y_test, dt_pred, labels=dt_model.classes_)\n",
    "    print(classification_report(y_test, dt_pred))\n",
    "    print(\"Accuracy on test set:\", acc)\n",
    "\n",
    "    return dt_model, dt_cm, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Burdan sonrasi komple GPT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge_data(accel_file, gyro_file, sample_rate):\n",
    "    # Load accelerometer data\n",
    "    accel_data = pd.read_csv(accel_file)\n",
    "    gyro_data = pd.read_csv(gyro_file)\n",
    "\n",
    "    # Assuming columns are: time, x, y, z\n",
    "    # Rename for clarity: ax, ay, az for accel; gx, gy, gz for gyro\n",
    "    # Convert time to datetime if needed\n",
    "    accel_data['timestamp'] = pd.to_datetime(accel_data['time'])\n",
    "    gyro_data['timestamp'] = pd.to_datetime(gyro_data['time'])\n",
    "\n",
    "    accel_data.set_index('timestamp', inplace=True)\n",
    "    gyro_data.set_index('timestamp', inplace=True)\n",
    "\n",
    "    # Resample both to ensure same frequency and align times\n",
    "    accel_data = accel_data.resample(f'{1/sample_rate}S').mean().interpolate()\n",
    "    gyro_data = gyro_data.resample(f'{1/sample_rate}S').mean().interpolate()\n",
    "\n",
    "    # Merge on nearest timestamps\n",
    "    merged = pd.merge_asof(accel_data.sort_index(), gyro_data.sort_index(),\n",
    "                           left_index=True, right_index=True, direction='nearest',\n",
    "                           suffixes=('_accel','_gyro'))\n",
    "\n",
    "    # Rename columns to standard names\n",
    "    merged.rename(columns={\n",
    "        'x_accel': 'ax', 'y_accel': 'ay', 'z_accel': 'az',\n",
    "        'x_gyro': 'gx', 'y_gyro': 'gy', 'z_gyro': 'gz'\n",
    "    }, inplace=True)\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_combined_features(data, window_sec, sample_rate, position):\n",
    "    # Compute accel magnitude and remove noise\n",
    "    data = calc_magnitude(data)\n",
    "    data = remove_noise(data, sample_rate)\n",
    "\n",
    "    # Compute orientation from accelerometer\n",
    "    # The calc_orientation expects 'x','y','z' columns, so map ax,ay,az\n",
    "    data['x'] = data['ax']\n",
    "    data['y'] = data['ay']\n",
    "    data['z'] = data['az']\n",
    "    data = calc_orientation(data)\n",
    "\n",
    "    # Now we have accel_mag, filtered_accel_mag, pitch, roll\n",
    "    # Also have gyro data: gx, gy, gz\n",
    "\n",
    "    # Add gyro magnitude if needed\n",
    "    data['gyro_mag'] = np.sqrt(data['gx']**2 + data['gy']**2 + data['gz']**2)\n",
    "\n",
    "    # Resample into windows\n",
    "    data['timestamp'] = data.index\n",
    "    data.set_index('timestamp', inplace=True)\n",
    "\n",
    "    window_size = f'{window_sec}s'\n",
    "    resampled_data = data.resample(window_size)\n",
    "\n",
    "    features_df = pd.DataFrame()\n",
    "\n",
    "    for timestamp, window in resampled_data:\n",
    "        if not window.empty:\n",
    "            # Basic accel features\n",
    "            base_features = add_features(window)  # avg, max, med, min, q25, q75, std (from filtered_accel_mag)\n",
    "\n",
    "            # Orientation features\n",
    "            orientation_feats = {\n",
    "                'pitch_mean': window['pitch'].mean(),\n",
    "                'pitch_std': window['pitch'].std(),\n",
    "                'roll_mean': window['roll'].mean(),\n",
    "                'roll_std': window['roll'].std()\n",
    "            }\n",
    "\n",
    "            # Stability and movement intensity\n",
    "            # stability = std/mean of filtered_accel_mag\n",
    "            mean_val = window['filtered_accel_mag'].mean() if window['filtered_accel_mag'].mean() != 0 else 1e-9\n",
    "            stability = window['filtered_accel_mag'].std() / mean_val\n",
    "            movement_intensity = np.sum(np.abs(np.diff(window['filtered_accel_mag'])))\n",
    "\n",
    "            extra_feats = {\n",
    "                'stability': stability,\n",
    "                'movement_intensity': movement_intensity\n",
    "            }\n",
    "\n",
    "            combined_features = {**base_features.iloc[0], **orientation_feats, **extra_feats}\n",
    "            combined_features['position'] = position\n",
    "\n",
    "            features_df = pd.concat([features_df, pd.DataFrame([combined_features])], ignore_index=True)\n",
    "\n",
    "    return features_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_combined_data(accel_root, gyro_root, output_filename=\"combined_sleep_data.csv\", window_sec=5, sample_rate=100):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    for position in ['back', 'side', 'stomach']:\n",
    "        accel_files = glob.glob(os.path.join(accel_root, position, '*.csv'))\n",
    "\n",
    "        for accel_file in accel_files:\n",
    "            filename = os.path.basename(accel_file)\n",
    "            gyro_file = os.path.join(gyro_root, position, filename)\n",
    "\n",
    "            if not os.path.exists(gyro_file):\n",
    "                print(f\"No matching gyro file for {accel_file}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            merged_data = load_and_merge_data(accel_file, gyro_file, sample_rate)\n",
    "\n",
    "            features_df = extract_combined_features(merged_data, window_sec, sample_rate, position)\n",
    "            all_data = pd.concat([all_data, features_df], ignore_index=True)\n",
    "\n",
    "    all_data.to_csv(output_filename, index=False)\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_position_classifier(frames):\n",
    "    # Ensure the needed features are present\n",
    "    features = [\n",
    "        'avg', 'max', 'med', 'min', 'q25', 'q75', 'std',\n",
    "        'pitch_mean', 'pitch_std', 'roll_mean', 'roll_std',\n",
    "        'stability', 'movement_intensity'\n",
    "    ]\n",
    "\n",
    "    X = frames[features]\n",
    "    y = frames['position']\n",
    "\n",
    "    # Stratify to maintain position distribution\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    dt_model = DecisionTreeClassifier(\n",
    "        criterion='entropy',\n",
    "        max_depth=6,\n",
    "        min_samples_leaf=5\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "    dt_pred = dt_model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, dt_pred)\n",
    "    print(classification_report(y_test, dt_pred))\n",
    "    print(\"Accuracy on test set:\", acc)\n",
    "\n",
    "    dt_cm = confusion_matrix(y_test, dt_pred, labels=dt_model.classes_)\n",
    "\n",
    "    return dt_model, dt_cm, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching gyro file for ./data/acceloremeter/back/CerenBackAcc.csv, skipping.\n",
      "No matching gyro file for ./data/acceloremeter/back/CerenBackAcc8.csv, skipping.\n",
      "No matching gyro file for ./data/acceloremeter/back/CerenBackAcc9.csv, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/vp82jcv55c5bkh9glyhptbsc0000gn/T/ipykernel_51130/4041522119.py:16: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  accel_data = accel_data.resample(f'{1/sample_rate}S').mean().interpolate()\n",
      "/var/folders/nd/vp82jcv55c5bkh9glyhptbsc0000gn/T/ipykernel_51130/4041522119.py:17: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  gyro_data = gyro_data.resample(f'{1/sample_rate}S').mean().interpolate()\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'x'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_sleep_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Process data and extract features\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m all_data \u001b[38;5;241m=\u001b[39m process_combined_data(accel_root, gyro_root, output_file, window_sec\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, sample_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Train the classifier\u001b[39;00m\n\u001b[1;32m      9\u001b[0m dt_model, dt_cm, acc \u001b[38;5;241m=\u001b[39m train_position_classifier(all_data)\n",
      "Cell \u001b[0;32mIn[123], line 17\u001b[0m, in \u001b[0;36mprocess_combined_data\u001b[0;34m(accel_root, gyro_root, output_filename, window_sec, sample_rate)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         merged_data \u001b[38;5;241m=\u001b[39m load_and_merge_data(accel_file, gyro_file, sample_rate)\n\u001b[0;32m---> 17\u001b[0m         features_df \u001b[38;5;241m=\u001b[39m extract_combined_features(merged_data, window_sec, sample_rate, position)\n\u001b[1;32m     18\u001b[0m         all_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([all_data, features_df], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m all_data\u001b[38;5;241m.\u001b[39mto_csv(output_filename, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[122], line 3\u001b[0m, in \u001b[0;36mextract_combined_features\u001b[0;34m(data, window_sec, sample_rate, position)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_combined_features\u001b[39m(data, window_sec, sample_rate, position):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Compute accel magnitude and remove noise\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     data \u001b[38;5;241m=\u001b[39m calc_magnitude(data)\n\u001b[1;32m      4\u001b[0m     data \u001b[38;5;241m=\u001b[39m remove_noise(data, sample_rate)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Compute orientation from accelerometer\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# The calc_orientation expects 'x','y','z' columns, so map ax,ay,az\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[113], line 4\u001b[0m, in \u001b[0;36mcalc_magnitude\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_magnitude\u001b[39m(data):\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Calculate magnitude\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccel_mag\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# absolute accel magnitude\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccel_mag\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccel_mag\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccel_mag\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;66;03m# detrend: \"remove gravity\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'x'"
     ]
    }
   ],
   "source": [
    "accel_root = './data/acceloremeter'\n",
    "gyro_root = './data/gyroscope'\n",
    "output_file = \"combined_sleep_data.csv\"\n",
    "\n",
    "# Process data and extract features\n",
    "all_data = process_combined_data(accel_root, gyro_root, output_file, window_sec=5, sample_rate=100)\n",
    "\n",
    "# Train the classifier\n",
    "dt_model, dt_cm, acc = train_position_classifier(all_data)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", dt_cm)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "position_labels = all_data['position'].unique()\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(dt_cm, annot=True, fmt='d', cmap='Blues', xticklabels=position_labels, yticklabels=position_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix for Sleep Position Classification')\n",
    "plt.show()\n",
    "\n",
    "# Visualize feature importance\n",
    "features = [\n",
    "    'avg', 'max', 'med', 'min', 'q25', 'q75', 'std',\n",
    "    'pitch_mean', 'pitch_std', 'roll_mean', 'roll_std',\n",
    "    'stability', 'movement_intensity'\n",
    "]\n",
    "importances = dt_model.feature_importances_\n",
    "fi_df = pd.DataFrame({'Feature': features, 'Importance': importances}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Importance', y='Feature', data=fi_df)\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
